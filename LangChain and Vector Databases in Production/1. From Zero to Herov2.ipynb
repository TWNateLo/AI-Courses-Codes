{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LLMs\n",
    "The fundamental component of LangChain involves invoking an LLM with a specific input. To illustrate this, we'll explore a simple example. Let's imagine we are building a service that suggests personalized workout routines based on an individual's fitness goals and preferences.\n",
    "\n",
    "To accomplish this, we will first need to import the LLM wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.9.9) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temperature parameter in OpenAI models manages the randomness of the output. When set to 0, the output is mostly predetermined and suitable for tasks requiring stability and the most probable result. At a setting of 1.0, the output can be inconsistent and interesting but isn't generally advised for most tasks. For creative tasks, a temperature between 0.70 and 0.90 offers a balance of reliability and creativity. The best setting should be determined by experimenting with different values for each specific use case. The code initializes the GPT-3.5 model’s Turbo variant. We will learn more about the various models and their differences later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "Adding the environment variables\n",
    "\n",
    "https://platform.openai.com/docs/quickstart#:~:text=Right%2Dclick%20on%20'This%20PC,key%20as%20the%20variable%20value.\n",
    "\n",
    "https://www.immersivelimit.com/tutorials/adding-your-openai-api-key-to-system-environment-variables"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up example:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call it on some input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\llms\\base.py:338\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    336\u001b[0m     )\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([prompt], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    341\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\llms\\base.py:206\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    205\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    207\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager:\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\llms\\base.py:198\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    194\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m), prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[0;32m    195\u001b[0m )\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 198\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    199\u001b[0m             prompts, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    200\u001b[0m         )\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    205\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\llms\\openai.py:326\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    324\u001b[0m     choices\u001b[38;5;241m.\u001b[39mextend(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 326\u001b[0m     response \u001b[38;5;241m=\u001b[39m completion_with_retry(\u001b[38;5;28mself\u001b[39m, prompt\u001b[38;5;241m=\u001b[39m_prompts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    327\u001b[0m     choices\u001b[38;5;241m.\u001b[39mextend(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\llms\\openai.py:106\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\tenacity\\__init__.py:330\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\n\u001b[0;32m    327\u001b[0m     f, functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__defaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__kwdefaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    328\u001b[0m )\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    476\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"Suggest a personalized workout routine for someone looking to improve cardiovascular endurance and prefers outdoor activities.\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will generate a personalized workout routine based on the user's fitness goals and preferences using the LLM model:\n",
    "\n",
    "1) Monday: Jog for 30 minutes outside \n",
    "2) Tuesday: 30 minutes of jumping jacks \n",
    "3) Wednesday: 30 minutes of running stairs \n",
    "4) Thursday: 30 minutes of sprints \n",
    "5) Friday: 30 minutes of jogging on an inclined treadmill \n",
    "6) Saturday: 30 minutes of interval training \n",
    "7) Sunday: Walk for 30 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chains\n",
    "\n",
    "In LangChain, a chain is an end-to-end wrapper around multiple individual components, providing a way to accomplish a common use case by combining these components in a specific sequence. The most commonly used type of chain is the LLMChain, which consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser.\n",
    "\n",
    "\n",
    "The LLMChain works as follows:\n",
    "\n",
    "\n",
    "Takes (multiple) input variables.\n",
    "Uses the PromptTemplate to format the input variables into a prompt.\n",
    "Passes the formatted prompt to the model (LLM or ChatModel).\n",
    "If an output parser is provided, it uses the OutputParser to parse the output of the LLM into a final format.\n",
    "In the next example, we demonstrate how to create a chain that generates a possible name for a company that produces eco-friendly water bottles. By using LangChain's LLMChain, PromptTemplate, and OpenAIclasses, we can easily define our prompt, set the input variables, and generate creative outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"eco-friendly water bottles\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output:\n",
    "\n",
    "Eco-Pure Water Bottles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Memory\n",
    "\n",
    "In LangChain, Memory refers to the mechanism that stores and manages the conversation history between a user and the AI. It helps maintain context and coherency throughout the interaction, enabling the AI to generate more relevant and accurate responses. Memory, such as ConversationBufferMemory, acts as a wrapper around ChatMessageHistory, extracting the messages and providing them to the chain for better context-aware generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "\n",
    "# Start the conversation\n",
    "conversation.predict(input=\"Tell me about yourself.\")\n",
    "\n",
    "# Continue the conversation\n",
    "conversation.predict(input=\"What can you do?\")\n",
    "conversation.predict(input=\"How can you help me with data analysis?\")\n",
    "\n",
    "# Display the conversation\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output you will observe resembles the following, although the specifics may be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**> Entering new ConversationChain chain...**\n",
    "\n",
    "Prompt after formatting:\n",
    "\n",
    "***The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "Human: Tell me about yourself.\n",
    "AI:  Hi there! I'm an AI created to help people with their daily tasks. I'm programmed to understand natural language and respond to questions and commands. I'm also able to learn from my interactions with people, so I'm constantly growing and improving. I'm excited to help you out!\n",
    "Human: What can you do?\n",
    "AI:  I can help you with a variety of tasks, such as scheduling appointments, setting reminders, and providing information. I'm also able to answer questions about topics like current events, sports, and entertainment. I'm always learning new things, so I'm sure I can help you with whatever you need.\n",
    "Human: How can you help me with data analysis?\n",
    "AI:*  I'm not familiar with data analysis, but I'm sure I can help you find the information you need. I can search the web for articles and resources related to data analysis, and I can also provide you with links to helpful websites.**\n",
    "\n",
    "**> Finished chain.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this output, you can see the memory being used by observing the \"Current conversation\" section. After each input from the user, the conversation is updated with both the user's input and the AI's response. This way, the memory maintains a record of the entire conversation. When the AI generates its next response, it will use this conversation history as context, making its responses more coherent and relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Lake VectorStore\n",
    "\n",
    "Deep Lake provides storage for embeddings and their corresponding metadata in the context of LLM apps. It enables hybrid searches on these embeddings and their attributes for efficient data retrieval. It also integrates with LangChain, facilitating the development and deployment of applications.\n",
    "\n",
    "\n",
    "Deep Lake provides several advantages over the typical vector store:\n",
    "\n",
    "\n",
    "- It’s multimodal, which means that it can be used to store items of diverse modalities, such as texts, images, audio, and video, along with their vector representations. \n",
    "- It’s serverless, which means that we can create and manage cloud datasets without creating and managing a database instance. This aspect gives a great speedup to new projects.\n",
    "- Last, it’s possible to easily create a data loader out of the data loaded into a Deep Lake dataset. It is convenient for fine-tuning machine learning models using common frameworks like PyTorch and TensorFlow.\n",
    "\n",
    "\n",
    "In order to use Deep Lake, you first have to register on the Activeloop website and redeem your API token. Here are the steps for doing it:\n",
    "\n",
    "\n",
    "1. Sign up for an account on Activeloop's platform. You can sign up at Activeloop's website. After specifying your username, click on the “Sign up” button. You should now see your homepage.\n",
    "2. You should now see a “Create API token” button at the top of your homepage. Click on it, and you’ll get redirected to the “API tokens” page. This is where you can generate, manage, and revoke your API keys for accessing Deep Lake.\n",
    "3. Click on the \"Create API token\" button. Then, you should see a popup asking for a token name and an expiration date. By default, the token expiration date is set so that the token expires after one day from its creation, but you can set it further in the future if you want to keep using the same token for the whole duration of the course. Once you’ve set the token name and its expiration date, click on the “Create API token” button.\n",
    "4. You should now see a green banner saying that the token has been successfully generated, along with your new API token, on the “API tokens” page. To copy your token to your clipboard, click on the square icon on its right.\n",
    "Now that you have your API token, you can conveniently store under the ACTIVELOOP_TOKEN key in the environment variable to retrieve it automatically by the Deep Lake libraries whenever needed. You can also save the token to its environment variable with Python, like in the following code snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import os\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = \"<YOUR-ACTIVELOOP-TOKEN>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = \"eyJhbGciOiJub25lIiwidHlwIjoiSldUIn0.eyJpZCI6InR3bmF0ZWxvIiwiYXBpX2tleSI6InZrMVhnOFYxS2phR0szYTQ1MjlnZ3RTOGJQbnhCbzNFTW9YVjZ2TkhONmJEViJ9.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that the previous code snippet adds the environment variable only in the context of the current code execution. Therefore, you’d need to run it again for each new Python program you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set your API keys/tokens in your code explicitly, please be careful in doing so, because if another person gets your API key (e.g., by seeing it in a GitHub repository where you uploaded it by mistake), then they can do actions on your behalf, as the API key is associated to your account. Exposing these sensitive credentials directly in the code poses a security risk. In case of a key compromise or a need to revoke access, developers would have to modify and redeploy the code, which can be time-consuming and error-prone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's best to store API keys in secure configuration files or use environment variables to keep them separate from the codebase, reducing the risk of accidental exposure and simplifying key management processes.\n",
    "\n",
    "Let’s install the deeplake library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "!pip install deeplake\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment and load the data. Before executing the following code, save your Activeloop key in the “ACTIVELOOP_TOKEN” environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 2 embeddings in 1 batches of size 2::   0%|          | 0/1 [00:00<?, ?it/s]Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors..\n",
      "Creating 2 embeddings in 1 batches of size 2::   0%|          | 0/1 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m db \u001b[38;5;241m=\u001b[39m DeepLake(dataset_path\u001b[38;5;241m=\u001b[39mdataset_path, embedding_function\u001b[38;5;241m=\u001b[39membeddings)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# add documents to our Deep Lake dataset\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\vectorstores\\base.py:72\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m     71\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\vectorstores\\deeplake.py:184\u001b[0m, in \u001b[0;36mDeepLake.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [{}] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(texts))\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    185\u001b[0m     text\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    186\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadatas,\n\u001b[0;32m    187\u001b[0m     embedding_data\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    188\u001b[0m     embedding_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    189\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function\u001b[38;5;241m.\u001b[39membed_documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     return_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    193\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\deeplake\\core\\vectorstore\\deeplake_vectorstore.py:229\u001b[0m, in \u001b[0;36mVectorStore.add\u001b[1;34m(self, embedding_function, embedding_data, embedding_tensor, return_ids, rate_limiter, **tensors)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    152\u001b[0m     embedding_function: Optional[Union[Callable, List[Callable]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtensors,\n\u001b[0;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Adding elements to deeplake vector store.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    Tensor names are specified as parameters, and data for each tensor is specified as parameter values. All data must of equal length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m        Optional[List[str]]: List of ids if ``return_ids`` is set to True. Otherwise, None.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_handler\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    230\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[0;32m    231\u001b[0m         embedding_data\u001b[38;5;241m=\u001b[39membedding_data,\n\u001b[0;32m    232\u001b[0m         embedding_tensor\u001b[38;5;241m=\u001b[39membedding_tensor,\n\u001b[0;32m    233\u001b[0m         return_ids\u001b[38;5;241m=\u001b[39mreturn_ids,\n\u001b[0;32m    234\u001b[0m         rate_limiter\u001b[38;5;241m=\u001b[39mrate_limiter,\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtensors,\n\u001b[0;32m    236\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\deeplake\\core\\vectorstore\\dataset_handlers\\client_side_dataset_handler.py:139\u001b[0m, in \u001b[0;36mClientSideDH.add\u001b[1;34m(self, embedding_function, embedding_data, embedding_tensor, return_ids, rate_limiter, **tensors)\u001b[0m\n\u001b[0;32m    133\u001b[0m processed_tensors, id_ \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mpreprocess_tensors(\n\u001b[0;32m    134\u001b[0m     embedding_data, embedding_tensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtensors\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m id_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend_or_ingest_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrate_limiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate_limiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:535\u001b[0m, in \u001b[0;36mextend_or_ingest_dataset\u001b[1;34m(processed_tensors, dataset, embedding_function, embedding_tensor, embedding_data, rate_limiter, logger)\u001b[0m\n\u001b[0;32m    533\u001b[0m rate_limiter \u001b[38;5;241m=\u001b[39m populate_rate_limiter(rate_limiter)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# TODO: Add back the old logic with checkpointing after indexing is fixed\u001b[39;00m\n\u001b[1;32m--> 535\u001b[0m \u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessed_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrate_limiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:481\u001b[0m, in \u001b[0;36mextend\u001b[1;34m(embedding_function, embedding_data, embedding_tensor, processed_tensors, dataset, rate_limiter, _extend_batch_size, logger)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(embedding_data[\u001b[38;5;241m0\u001b[39m]), _extend_batch_size),\n\u001b[0;32m    477\u001b[0m     progressbar_str,\n\u001b[0;32m    478\u001b[0m ):\n\u001b[0;32m    479\u001b[0m     batch_start, batch_end \u001b[38;5;241m=\u001b[39m idx, idx \u001b[38;5;241m+\u001b[39m _extend_batch_size\n\u001b[1;32m--> 481\u001b[0m     batched_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_batched_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedding_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrate_limiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m     batched_tensors \u001b[38;5;241m=\u001b[39m _slice_non_embedding_tensors(\n\u001b[0;32m    491\u001b[0m         processed_tensors, embedding_tensor, batch_start, batch_end\n\u001b[0;32m    492\u001b[0m     )\n\u001b[0;32m    494\u001b[0m     batched_processed_tensors \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatched_embeddings, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatched_tensors}\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\deeplake\\core\\vectorstore\\vector_search\\dataset\\dataset.py:422\u001b[0m, in \u001b[0;36m_compute_batched_embeddings\u001b[1;34m(embedding_function, embedding_data, embedding_tensor, start_idx, end_idx, rate_limiter)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m func, data, tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(embedding_function, embedding_data, embedding_tensor):\n\u001b[0;32m    421\u001b[0m     data_slice \u001b[38;5;241m=\u001b[39m data[start_idx:end_idx]\n\u001b[1;32m--> 422\u001b[0m     embedded_data \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate_limiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrate_limiter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    425\u001b[0m         return_embedded_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(embedded_data)\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\deeplake\\core\\vectorstore\\embeddings\\embedder.py:77\u001b[0m, in \u001b[0;36mDeepLakeEmbedder.embed_documents\u001b[1;34m(self, documents, rate_limiter)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rate_limiter[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_rate_limiter(documents, embedding_func, rate_limiter)\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membedding_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\embeddings\\openai.py:305\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeployment\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\embeddings\\openai.py:243\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    241\u001b[0m _chunk_size \u001b[38;5;241m=\u001b[39m chunk_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens), _chunk_size):\n\u001b[1;32m--> 243\u001b[0m     response \u001b[38;5;241m=\u001b[39m embed_with_retry(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size],\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params,\n\u001b[0;32m    247\u001b[0m     )\n\u001b[0;32m    248\u001b[0m     batched_embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m    250\u001b[0m results: List[List[List[\u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(texts))]\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\langchain\\embeddings\\openai.py:64\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _embed_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\tenacity\\__init__.py:330\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\n\u001b[0;32m    327\u001b[0m     f, functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__defaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__kwdefaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    328\u001b[0m )\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\tenacity\\__init__.py:477\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoSleep):\n\u001b[0;32m    476\u001b[0m     retry_state\u001b[38;5;241m.\u001b[39mprepare_for_next_attempt()\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m do\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\tenacity\\nap.py:31\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(seconds)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msleep\u001b[39m(seconds: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Sleep strategy that delays execution for a given number of seconds.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    This is the default strategy, and may be mocked out for unit testing.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Before executing the following code, make sure to have your\n",
    "# Activeloop key saved in the “ACTIVELOOP_TOKEN” environment variable.\n",
    "\n",
    "# instantiate the LLM and embeddings models\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create our documents\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"twnatelo\" \n",
    "my_activeloop_dataset_name = \"langchain_course_from_zero_to_hero\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything works correctly, you should see a printed output like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Deep Lake dataset has been successfully created!\n",
    "The dataset is private so make sure you are logged in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If so, you’ve just created your first Deep Lake dataset!\n",
    "\n",
    "Now, let's create a RetrievalQA chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "\tllm=llm,\n",
    "\tchain_type=\"stuff\",\n",
    "\tretriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create an agent that uses the RetrievalQA chain as a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "\ttools,\n",
    "\tllm,\n",
    "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "\tverbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the agent to ask a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.run(\"When was Napoleone born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You should see something similar to the following printed output. Here, the agent used the “Retrieval QA System” tool with the query “When was Napoleone born?” which is then run on our new Deep Lake dataset, returning the most similar document (i.e., the document containing the date of birth of Napoleon). This document is eventually used to generate the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**> Entering new AgentExecutor chain...**\n",
    "\n",
    "***I need to find out when Napoleone was born.\n",
    "Action: Retrieval QA System\n",
    "Action Input: When was Napoleone born?***\n",
    "\n",
    "Observation:\n",
    "\n",
    "***Napoleon Bonaparte was born on 15 August 1769.***\n",
    "\n",
    "Thought:\n",
    "\n",
    "***I now know the final answer.\n",
    "Final Answer: Napoleon Bonaparte was born on 15 August 1769.***\n",
    "\n",
    "**> Finished chain.**\n",
    "\n",
    "Napoleon Bonaparte was born on 15 August 1769."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to use Deep Lake as a vector database and create an agent with a RetrievalQA chain as a tool to answer questions based on the given document.\n",
    "\n",
    "Let’s add an example of reloading an existing vector store and adding more data.\n",
    "\n",
    "We first reload an existing vector store from Deep Lake that's located at a specified dataset path. Then, we load new textual data and split it into manageable chunks. Finally, we add these chunks to the existing dataset, creating and storing corresponding embeddings for each added text segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the existing Deep Lake dataset and specify the embedding function\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# create new documents\n",
    "texts = [\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then recreate our previous agent and ask a question that can be answered only by the last documents added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the wrapper class for GPT3\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "# create a retriever from the db\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "\tllm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# instantiate a tool that uses the retriever\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# create an agent that uses the tool\n",
    "agent = initialize_agent(\n",
    "\ttools,\n",
    "\tllm,\n",
    "\tagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "\tverbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now test our agent with a new question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.run(\"When was Michael Jordan born?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printed output will look like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Entering new AgentExecutor chain...\n",
    " I need to find out when Michael Jordan was born.\n",
    "Action: Retrieval QA System\n",
    "Action Input: When was Michael Jordan born?\n",
    "\n",
    "Observation:  Michael Jordan was born on February 17, 1963.\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: Michael Jordan was born on February 17, 1963.\n",
    "\n",
    "> Finished chain.\n",
    "Michael Jordan was born on February 17, 1963."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The LLM successfully retrieves accurate information by using the power of Deep Lake as a vector store and the OpenAI language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents in LangChain\n",
    "In LangChain, agents are high-level components that use language models (LLMs) to determine which actions to take and in what order. An action can either be using a tool and observing its output or returning it to the user. Tools are functions that perform specific duties, such as Google Search, database lookups, or Python REPL.\n",
    "\n",
    "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
    "\n",
    "Several types of agents are available in LangChain:\n",
    "\n",
    "- The zero-shot-react-description agent uses the ReAct framework to decide which tool to employ based purely on the tool's description. It necessitates a description of each tool.\n",
    "- The react-docstore agent engages with a docstore through the ReAct framework. It needs two tools: a Search tool and a Lookup tool. The Search tool finds a document, and the Lookup tool searches for a term in the most recently discovered document.\n",
    "- The self-ask-with-search agent employs a single tool named Intermediate Answer, which is capable of looking up factual responses to queries. It is identical to the original self-ask with the search paper, where a Google search API was provided as the tool.\n",
    "- The conversational-react-description agent is designed for conversational situations. It uses the ReAct framework to select a tool and uses memory to remember past conversation interactions.\n",
    "In our example, the Agent will use the Google Search tool to look up recent information about the Mars rover and generates a response based on this information.\n",
    "\n",
    "First, you want to set the environment variables “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use Google Search via API. Refer to this article for a guide on how to get them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let’s import the necessary modules:\n",
    "\n",
    "- langchain.llms.OpenAI: This is used to create an instance of the OpenAI language model, which can generate human-like text based on the input it's given.\n",
    "- langchain.agents.load_tools: This function is used to load a list of tools that an AI agent can use.\n",
    "- langchain.agents.initialize_agent: This function initializes an AI agent that can use a given set of tools and a language model to interact with users.\n",
    "- langchain.agents.Tool: This is a class used to define a tool that an AI agent can use. A tool is defined by its name, a function that performs the tool's action, and a description of the tool.\n",
    "- langchain.utilities.GoogleSearchAPIWrapper: This class is a wrapper for the Google Search API, allowing it to be used as a tool by an AI agent. It likely contains a method that sends a search query to Google and retrieves the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll initialize the LLM and set the temperature to 0 for the precise answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the Google search wrapper as follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to set the environment variables\n",
    "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
    "# Google Search via API.\n",
    "search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tool object represents a specific capability or function the system can use. In this case, it's a tool for performing Google searches.\n",
    "\n",
    "It is initialized with three parameters:\n",
    "\n",
    "- name parameter: This is a string that serves as a unique identifier for the tool. In this case, the name of the tool is \"google-search.”\n",
    "- func parameter: This parameter is assigned the function that the tool will execute when called. In this case, it's the run method of the search object, which presumably performs a Google search.\n",
    "- description parameter: This is a string that briefly explains what the tool does. The description explains that this tool is helpful when you need to use Google to answer questions about current events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name = \"google-search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to search google to answer questions about current events\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an agent that uses our Google Search tool:\n",
    "\n",
    "- initialize_agent(): This function call creates and initializes an agent. An agent is a component that determines which actions to take based on user input. These actions can be using a tool, returning a response to the user, or something else.\n",
    "- tools:  represents the list of Tool objects that the agent can use.\n",
    "- agent=\"zero-shot-react-description\": The \"zero-shot-react-description\" type of an Agent uses the ReAct framework to decide which tool to use based only on the tool's description.\n",
    "- verbose=True: when set to True, it will cause the Agent to print more detailed information about what it's doing. This is useful for debugging and understanding what's happening under the hood.\n",
    "- max_iterations=6: sets a limit on the number of iterations the Agent can perform before stopping. It's a way of preventing the agent from running indefinitely in some cases, which may have unwanted monetary costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True,\n",
    "                         max_iterations=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we can check out the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent(\"What's the latest news about the Mars rover?\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll see an output like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Entering new AgentExecutor chain...\n",
    " I need to find out the latest news about the Mars rover\n",
    "Action: google-search\n",
    "Action Input: \"latest news Mars rover\"\n",
    "Observation: The mission has concluded that the solar-powered lander has run out of energy after more than four years on the Red Planet. Dec 15, 2021 ... Mars Sample Return is going to have great stuff to choose from!” Get the Latest JPL News. SUBSCRIBE TO THE NEWSLETTER. NASA's Curiosity rover is discovering odd rocks halfway into a region called the \"clay-bearing unit.\" ... This panorama of a location called \"Teal Ridge\" was ... 2 days ago ... LEGO Technic 42158 NASA Mars Rover Perseverance officially revealed as latest real-life space set [News]. Today LEGO has officially revealed ... Oct 19, 2022 ... NASA's Curiosity Mars rover used its Mast Camera, or Mastcam, to capture this panorama of a hill nicknamed ... Get the Latest JPL News. Latest Updates · Curiosity rover finds water-carved 'book' rock on Mars (photo) · Curiosity rover on Mars gets a brain boost to think (and move) faster · Curiosity ... Mar 7, 2023 ... 27, helps provide scientists with information about the particle sizes within the clouds. Sign up for National Breaking News Alerts. Sep 16, 2022 ... Since July, NASA's Perseverance rover has drilled and collected four slim cores of sedimentary rock, formed in what was once a lake on Mars. Nasa Mars lander study reveals 'main source of heat' on Red Planet · NASA/ESA/A. Simon (Goddard Space Flight Center) and M.H. Wong. All the latest content about Nasa Perseverance Mars rover from the BBC. ... James Tytko presents science news and we're putting tuberculosis under the ...\n",
    "Thought: I now know the final answer\n",
    "Final Answer: The latest news about the Mars rover is that the mission has concluded that the solar-powered lander has run out of energy after more than four years on the Red Planet. NASA's Curiosity rover is discovering odd rocks halfway into a region called the \"clay-bearing unit.\" LEGO Technic 42158 NASA Mars Rover Perseverance has been officially revealed as the latest real-life space set. NASA's Curiosity Mars rover used its Mast Camera, or Mastcam, to capture a panorama of a hill nicknamed \"Teal Ridge.\" NASA's Perseverance rover has drilled and collected four slim cores of sedimentary rock, formed in what was once a lake on Mars. A study of the Mars lander has revealed the 'main source of heat' on the Red Planet.\n",
    "\n",
    "> Finished chain.\n",
    "The latest news about the Mars rover is that the mission has concluded that the solar-powered lander has run out of energy after more than four years on the Red Planet. NASA's Curiosity rover is discovering odd rocks halfway into a region called the \"clay-bearing unit.\" LEGO Technic 42158 NASA Mars Rover Perseverance has been officially revealed as the latest real-life space set. NASA's Curiosity Mars rover used its Mast Camera, or Mastcam, to capture a panorama of a hill nicknamed \"Teal Ridge.\" NASA's Perseverance rover has drilled and collected four slim cores of sedimentary rock, formed in what was once a lake on Mars. A study of the Mars lander has revealed the 'main source of heat' on the Red Planet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, Agents in LangChain help decide which actions to take based on user input. The example demonstrates initializing and using a \"zero-shot-react-description\" agent with a Google search tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain provides a variety of tools for agents to interact with the outside world. These tools can be used to create custom agents that perform various tasks, such as searching the web, answering questions, or running Python code. In this section, we will discuss the different tool types available in LangChain and provide examples of creating and using them.\n",
    "\n",
    "In our example, two tools are being defined for use within a LangChain agent: a Google Search tool and a Language Model tool acting specifically as a text summarizer. The Google Search tool, using the GoogleSearchAPIWrapper, will handle queries that involve finding recent event information. The Language Model tool leverages the capabilities of a language model to summarize texts. These tools are designed to be used interchangeably by the agent, depending on the nature of the user's query.\n",
    "\n",
    "Let’s import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then instantiate a  LLMChain specifically for text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Write a summary of the following text: {query}\"\n",
    ")\n",
    "\n",
    "summarize_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the tools that our agent will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to set the environment variables\n",
    "# “GOOGLE_API_KEY” and “GOOGLE_CSE_ID” to be able to use\n",
    "# Google Search via API.\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for finding information about recent events\"\n",
    "    ),\n",
    "    Tool(\n",
    "       name='Summarizer',\n",
    "       func=summarize_chain.run,\n",
    "       description='useful for summarizing texts'\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create our agent that leverages two tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run the agent with a question about summarizing the latest news about the Mars rover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent(\"What's the latest news about the Mars rover? Then please summarize the results.\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an output like the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Entering new AgentExecutor chain...\n",
    " I should search for recent news about the Mars rover and then summarize the results.\n",
    "Action: Search\n",
    "Action Input: Latest news about the Mars rover\n",
    "Observation: Mars 2020 Perseverance Rover · The largest and most capable rover ever sent to Mars. ... Curiosity Rover · Measures Mars' atmosphere to understand its climate ... Dec 15, 2021 ... Mars Sample Return is going to have great stuff to choose from!” Get the Latest JPL News. SUBSCRIBE TO THE NEWSLETTER. The mission has concluded that the solar-powered lander has run out of energy after more than four years on the Red Planet. Oct 19, 2022 ... NASA's Curiosity Mars rover used its Mast Camera, or Mastcam, to capture this panorama of a hill nicknamed ... Get the Latest JPL News. NASA's Mars 2020 Perseverance rover will look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. Latest Updates · Curiosity rover finds water-carved 'book' rock on Mars (photo) · Curiosity rover on Mars gets a brain boost to think (and move) faster · Curiosity ... Feb 8, 2023 ... Curiosity Rover Finds New Clues to Mars' Watery Past ... at Gediz Vallis Ridge twice last year but could only survey it from a distance. Sep 16, 2022 ... Since July, NASA's Perseverance rover has drilled and collected four slim cores of sedimentary rock, formed in what was once a lake on Mars. Mar 7, 2023 ... 27, helps provide scientists with information about the particle sizes within the clouds. Sign up for National Breaking News Alerts. All the latest content about Nasa Perseverance Mars rover from the BBC. ... James Tytko presents science news and we're putting tuberculosis under the ...\n",
    "Thought: I should summarize the results of the search.\n",
    "Action: Summarizer\n",
    "Action Input: Mars 2020 Perseverance Rover is the largest and most capable rover ever sent to Mars. It measures Mars' atmosphere to understand its climate and has run out of energy after more than four years on the Red Planet. NASA's Curiosity Mars rover used its Mast Camera to capture a panorama of a hill nicknamed \"Book Rock\". NASA's Mars 2020 Perseverance rover will look for signs of past microbial life, cache rock and soil samples, and prepare for future human exploration. Curiosity rover finds water-carved 'book' rock on Mars, Curiosity rover on Mars gets a brain boost to think (and move) faster, Curiosity Rover Finds New Clues to Mars' Watery Past, and NASA's Perseverance rover has drilled and collected four slim cores of sedimentary rock.\n",
    "Observation: \n",
    "\n",
    "NASA's Mars 2020 Perseverance rover is the largest and most capable rover ever sent to Mars. It has been on the Red Planet for more than four years, measuring the atmosphere to understand its climate and searching for signs of past microbial life. The Curiosity rover has captured a panorama of a hill nicknamed \"Book Rock\" and has been given a brain boost to think and move faster. It has also found new clues to Mars' watery past. The Perseverance rover has drilled and collected four slim cores of sedimentary rock, which will be used to cache rock and soil samples and prepare for future human exploration.\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: NASA's Mars 2020 Perseverance rover is the largest and most capable rover ever sent to Mars. It has been on the Red Planet for more than four years, measuring the atmosphere to understand its climate and searching for signs of past microbial life. The Curiosity rover has captured a panorama of a hill nicknamed \"Book Rock\" and has been given a brain boost to think and move faster. It has also found new clues to Mars' watery past. The Perseverance rover has drilled and collected four slim cores of sedimentary rock, which will be used to cache rock and soil samples and prepare for future human exploration.\n",
    "\n",
    "> Finished chain.\n",
    "NASA's Mars 2020 Perseverance rover is the largest and most capable rover ever sent to Mars. It has been on the Red Planet for more than four years, measuring the atmosphere to understand its climate and searching for signs of past microbial life. The Curiosity rover has captured a panorama of a hill nicknamed \"Book Rock\" and has been given a brain boost to think and move faster. It has also found new clues to Mars' watery past. The Perseverance rover has drilled and collected four slim cores of sedimentary rock, which will be used to cache rock and soil samples and prepare for future human exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the agents used at first the “Search” tool to look for recent information about the Mars rover and then used the “Summarizer” tool for writing a summary.\n",
    "\n",
    "LangChain provides an expansive toolkit that integrates various functions to improve the functionality of conversational agents. Here are some examples:\n",
    "\n",
    "- SerpAPI: This tool is an interface for the SerpAPI search engine, allowing the agent to perform robust online searches to pull in relevant data for a conversation or task.\n",
    "- PythonREPLTool: This unique tool enables the writing and execution of Python code within an agent. This opens up a wide range of possibilities for advanced computations and interactions within the conversation.\n",
    "If you wish to add more specialized capabilities to your LangChain conversational agent, the platform offers the flexibility to create custom tools. By following the general tool creation guidelines provided in the LangChain documentation, you can develop tools tailored to the specific needs of your application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As we conclude our comprehensive exploration of LangChain, it's clear that this toolset presents a remarkable opportunity for both novice and seasoned AI developers. Through our journey, we've unpacked key concepts, from library installation to configuring OpenAI credentials, and dove into the intricacies of generating predictions using language models.\n",
    "\n",
    "Whether you're just starting with LangChain or looking to enhance your existing AI workflow, we hope this exploration has provided you with a solid foundation to build upon.\n",
    "\n",
    "For a more in-depth explanation of all the LangChain concepts and several projects examples with it, consider taking the full course, starting with the lesson on large language models. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
