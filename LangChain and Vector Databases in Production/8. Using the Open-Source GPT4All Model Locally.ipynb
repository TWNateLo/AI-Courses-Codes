{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Open-Source GPT4All Model Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The course method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ChatGPT method (Should be easier and more efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying the GPT4ALL model for use with LangChain involves several steps. You'll need to install the necessary packages, download the GPT4ALL model, and integrate it with LangChain. Here is a step-by-step guide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install Necessary Packages\n",
    "\n",
    "Ensure you have the required packages installed. You will need gpt4all, langchain, and any other dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install gpt4all langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Download the GPT4ALL Model\n",
    "\n",
    "Download the GPT4ALL model from its official repository or website. You can usually find pre-trained models that you can download and use locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load and Deploy GPT4ALL Model\n",
    "\n",
    "Once you have the model downloaded, you can load it and deploy it using LangChain. Hereâ€™s an example of how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.9.10) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Request failed: HTTP 404 Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/your/gpt4all-model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the GPT4ALL model\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m gpt4all_model \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define a wrapper for the GPT4ALL model to work with LangChain\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGPT4AllLLM\u001b[39;00m(OpenAI):\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\gpt4all\\gpt4all.py:235\u001b[0m, in \u001b[0;36mGPT4All.__init__\u001b[1;34m(self, model_name, model_path, model_type, allow_download, n_threads, device, n_ctx, ngl, verbose)\u001b[0m\n\u001b[0;32m    232\u001b[0m         device_init \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkompute:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Retrieve model and download if allowed\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig: ConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LLModel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m], n_ctx, ngl, backend)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\gpt4all\\gpt4all.py:341\u001b[0m, in \u001b[0;36mGPT4All.retrieve_model\u001b[1;34m(cls, model_name, model_path, allow_download, verbose)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m allow_download:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;66;03m# If model file does not exist, download\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     filesize \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 341\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilesize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilesize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_md5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmd5sum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_dest\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\gpt4all\\gpt4all.py:393\u001b[0m, in \u001b[0;36mGPT4All.download_model\u001b[1;34m(model_filename, model_path, verbose, url, expected_size, expected_md5)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected identity Content-Encoding, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m--> 393\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m total_size_in_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    396\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# 1 MB\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nate.lo\\anaconda3\\envs\\Python39\\lib\\site-packages\\gpt4all\\gpt4all.py:386\u001b[0m, in \u001b[0;36mGPT4All.download_model.<locals>.make_request\u001b[1;34m(offset)\u001b[0m\n\u001b[0;32m    384\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m206\u001b[39m):\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRequest failed: HTTP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;129;01mand\u001b[39;00m (response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m206\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(offset) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Range\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConnection was interrupted and server does not support range requests\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Request failed: HTTP 404 Not Found"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gpt4all import GPT4All\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Path to the downloaded GPT4ALL model\n",
    "model_path = \"path/to/your/gpt4all-model.bin\"\n",
    "\n",
    "# Initialize the GPT4ALL model\n",
    "gpt4all_model = GPT4All(model_path)\n",
    "\n",
    "# Define a wrapper for the GPT4ALL model to work with LangChain\n",
    "class GPT4AllLLM(OpenAI):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        response = self.model.generate(prompt)\n",
    "        return response\n",
    "\n",
    "# Create an instance of the custom LLM with the GPT4ALL model\n",
    "llm = GPT4AllLLM(gpt4all_model)\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(template=\"Question: {question}\\nAnswer:\", input_variables=[\"question\"])\n",
    "\n",
    "# Create the LLMChain with the custom LLM and the prompt template\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain with your question\n",
    "response = chain.run({\"question\": \"What is the meaning of life?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "1. Model Path: Set the path to your downloaded GPT4ALL model.\n",
    "2. Initialize the Model: Load the GPT4ALL model using its provided API.\n",
    "3. Custom LLM Wrapper: Define a wrapper class GPT4AllLLM that adapts the GPT4ALL model to work with LangChain's OpenAI interface.\n",
    "4. Create an Instance: Create an instance of the custom LLM with the GPT4ALL model.\n",
    "5. Prompt Template: Define a prompt template for how you want to format your inputs.\n",
    "6. LLMChain: Create an LLMChain using the custom LLM and the prompt template.\n",
    "7. Run the Chain: Use the chain to generate responses based on your input questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Notes\n",
    "\n",
    "- Model Compatibility: Ensure the GPT4ALL model you download is compatible with the methods used in the GPT4All API.\n",
    "- Path Configuration: Adjust the model_path to the correct location of your GPT4ALL model file.\n",
    "- Dependencies: Ensure all dependencies are installed and correctly configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example provides a basic setup. Depending on your specific use case and the capabilities of the GPT4ALL model, you might need to adjust the implementation details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
