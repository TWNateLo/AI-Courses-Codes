{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Open-Source GPT4All Model Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The course method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Model and Generate\n",
    "\n",
    "The LangChain library uses PyLLaMAcpp module to load the converted GPT4All weights. Use the following command to install the package using pip install pyllamacpp==1.0.7 and import all the necessary functions. We will provide detailed explanations of the functions as they come up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by arguably the most essential part of interacting with LLMs is defining the prompt. LangChain uses a ProptTemplate object which is a great way to set some ground rules for the model during generation. For example, it is possible to show how we like the model to write. (called few-shot learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template string defines the interaction’s overall structure. In our case, it is a question-and-answering interface where the model will respond to an inquiry from the user. There are two important parts:\n",
    "\n",
    "1. Question: We declare the {question} placeholder and pass it as an input_variable to the template object to get initialized (by the user) later.\n",
    "2. Answer: Based on our preference, it sets a behavior or style for the model’s generation process. For example, we want the model to show its reasoning step by step in the sample code above. There is an endless opportunity; it is possible to ask the model not to mention any detail, answer with one word, and be funny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we set the expected behavior, it is time to load the model using the converted file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = GPT4All(model=\"./models/ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default behavior is to wait for the model to finish its inference process to print out its outputs. However, it could take more than an hour (depending on your hardware) to respond to one prompt because of the large number of parameters in the model. We can use the StreamingStdOutCallbackHandler() callback to instantly show the latest generated token. This way, we can be sure that the generation process is running and the model shows the expected behavior. Otherwise, it is possible to stop the inference and adjust the prompt.\n",
    "\n",
    "The GPT4All class is responsible for reading and initializing the weights file and setting the required callbacks. Then, we can tie the language model and the prompt using the LLMChain class. It will enable us to ask questions from the model using the run() object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What happens when it rains somewhere?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What happens when it rains somewhere?\n",
    "\n",
    "Answer: Let's think step by step. When rain falls, first of all, the water vaporizes \n",
    "from clouds and travels to a lower altitude where the air is denser. Then these drops \n",
    "hit surfaces like land or trees etc., which are considered as a target for this falling particle known as rainfall. This process continues till there's no more moisture \n",
    "available in that particular region, after which it stops being called rain (or \n",
    "precipitation) and starts to become dew/fog depending upon the ambient temperature & \n",
    "humidity of respective locations or weather conditions at hand. Question: What happens \n",
    "when it rains somewhere?\\n\\nAnswer: Let's think step by step. When rain falls, first of all, the water vaporizes from clouds and travels to a lower altitude where the air is \n",
    "denser. Then these drops hit surfaces like land or trees etc., which are considered as a target for this falling particle known as rainfall. This process continues till there's no more moisture available in that particular region, after which it stops being called rain (or precipitation) and starts to become dew/fog depending upon the ambient \n",
    "temperature & humidity of respective locations or weather conditions at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to test different prompt templates to find the best one that fits your needs. The following example asks the same question but expects the model to be funny while generating only two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What happens when it rains somewhere?\n",
    "\n",
    "Answer: Let's answer in two sentence while being funny. 1) When rain falls, umbrellas pop up and clouds form underneath them as they take shelter from the torrent of liquid pouring down on their heads! And...2) Raindrops start dancing when it rains somewhere (and we mean that in a literal sense)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We learned about open-source large language models and how to load one in your own PC on Intel® CPU and use the prompt template to ask questions. We also discussed the quantization process that makes this possible. In the next lesson, we will dive deeper and introduce more models while comparing them for different use cases.\n",
    "\n",
    "In the next lesson, you’ll see a comprehensive guide to the models that can be used with LangChain, along with a brief description of them.\n",
    "\n",
    "You can find the code of this lesson in this online notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LangChain Doc method (Should be easier and more efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://python.langchain.com/v0.2/docs/integrations/providers/gpt4all/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: Remember to install the package \"langchain-community\" manually with command [pip install langchain-community]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All\n",
    "\n",
    "To use the GPT4All wrapper, you need to provide the path to the pre-trained model file and the model's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "\n",
    "# Instantiate the model. Callbacks support token-wise streaming\n",
    "model = GPT4All(model=\"../GPT4ALL-Models/mistral-7b-openorca.gguf2.Q4_0.gguf\", n_threads=8)\n",
    "\n",
    "# Generate text\n",
    "response = model.invoke(\"Once upon a time, \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Reference:GPT4All\n",
    "\n",
    "You can also customize the generation parameters, such as n_predict, temp, top_p, top_k, and others.\n",
    "\n",
    "To stream the model's predictions, add in a CallbackManager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\Nate\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 years ago, I was in the process of moving to New York City. It was an exciting and scary time for me as it meant leaving my family and friends behind to start fresh in this big city.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'10 years ago, I was in the process of moving to New York City. It was an exciting and scary time for me as it meant leaving my family and friends behind to start fresh in this big city.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# There are many CallbackHandlers supported, such as\n",
    "# from langchain.callbacks.streamlit import StreamlitCallbackHandler\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "model = GPT4All(model=\"../GPT4ALL-Models/mistral-7b-openorca.gguf2.Q4_0.gguf\", n_threads=8)\n",
    "\n",
    "# Generate text. Tokens are streamed through the callback manager.\n",
    "model(\"Once upon a time, \", callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Reference:GPT4All | StreamingStdOutCallbackHandler | StreamlitCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model File\n",
    "\n",
    "You can find links to model file downloads in the https://gpt4all.io/.\n",
    "\n",
    "For a more detailed walkthrough of this, see this [notebook](https://python.langchain.com/v0.2/docs/integrations/llms/gpt4all/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Detailed Tutorials from the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub:nomic-ai/gpt4all an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.\n",
    "\n",
    "This example goes over how to use LangChain to interact with GPT4All models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Question to pass to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Model\n",
    "\n",
    "To run locally, download a compatible ggml-formatted model.\n",
    "\n",
    "The gpt4all page has a useful Model Explorer section:\n",
    "\n",
    "- Select a model of interest\n",
    "- Download using the UI and move the .bin to the local_path (noted below)\n",
    "\n",
    "For more info, visit https://github.com/nomic-ai/gpt4all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " First, we need to find out when Justin Bieber was born. He was born on March 1, 1994. Now, let's see which NFL team won the Super Bowl in that year. The answer is the San Francisco 49ers who won the Super Bowl XXIX against the San Diego Chargers.\n",
      "\n",
      "Justin Bieber was born in London, Ontario, Canada on March 1, 1994. He is a famous Canadian singer and songwriter known for his pop music. His full name is Justin Drew Bieber. He started singing at a very young age and gained popularity using the internet to showcase his talent. In 2008, he was discovered by American manager Scooter Braun who helped him sign a record deal. Since then, Bieber has released several albums and singles that have become chart-topping hits worldwide. Some of his popular songs include \"Baby,\" \"Sorry,\" and \"What Do You Mean?\"\n",
      "\n",
      "The San Francisco 49ers are an American football team based in the San Francisco Bay Area. They compete in the National Football League (NFL) as a member club of the league's National Football Conference"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' First, we need to find out when Justin Bieber was born. He was born on March 1, 1994. Now, let\\'s see which NFL team won the Super Bowl in that year. The answer is the San Francisco 49ers who won the Super Bowl XXIX against the San Diego Chargers.\\n\\nJustin Bieber was born in London, Ontario, Canada on March 1, 1994. He is a famous Canadian singer and songwriter known for his pop music. His full name is Justin Drew Bieber. He started singing at a very young age and gained popularity using the internet to showcase his talent. In 2008, he was discovered by American manager Scooter Braun who helped him sign a record deal. Since then, Bieber has released several albums and singles that have become chart-topping hits worldwide. Some of his popular songs include \"Baby,\" \"Sorry,\" and \"What Do You Mean?\"\\n\\nThe San Francisco 49ers are an American football team based in the San Francisco Bay Area. They compete in the National Football League (NFL) as a member club of the league\\'s National Football Conference'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path = (\n",
    "    \"../GPT4ALL-Models/mistral-7b-openorca.gguf2.Q4_0.gguf\"  # replace with your desired local file path\n",
    ")\n",
    "\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n",
    "\n",
    "# If you want to use a custom model add the backend parameter\n",
    "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
    "#llm = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Ku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\Nate\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\User\\anaconda3\\envs\\Nate\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "System: Of course! The answer for the mathematical expression \"1 + 3\" is 4. This is because when we add one and three together, we get a total of four. In other words, it's like having one apple and then adding two more apples to that amount; you would have a total of four apples.\n",
      " >>>>>>>>>>>>>>>>>>>>>>>> Result: \n",
      "\n",
      "{'question': 'Could you please tell me What is the answer of 1 + 3', 'answer': '?\\nSystem: Of course! The answer for the mathematical expression \"1 + 3\" is 4. This is because when we add one and three together, we get a total of four. In other words, it\\'s like having one apple and then adding two more apples to that amount; you would have a total of four apples.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate,ChatPromptTemplate,SystemMessagePromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = GPT4All(model=\"../GPT4ALL-Models/mistral-7b-openorca.gguf2.Q4_0.gguf\",callbacks=callbacks, verbose=True)\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"question\"], \n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template('You always try to explain things in detail'),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt = prompt,\n",
    "    output_key ='answer'\n",
    ")\n",
    "result = chain({\n",
    "    \"question\":\"Could you please tell me What is the answer of 1 + 3\", \n",
    "})\n",
    "print(\"\\n >>>>>>>>>>>>>>>>>>>>>>>> Result: \\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We have two numbers here, 1 and 3. When we add these together, we get:\n",
      "\n",
      "1 (the first number) + 3 (the second number) = 4\n",
      "\n",
      "So, the answer to \"What is the sum of 1 and 3?\" is 4.\n",
      " >>>>>>>>>>>>>>>>>>>>>>>> Result: \n",
      "\n",
      "{'question': 'Could you please tell me What is the answer of 1 + 3', 'answer': ' We have two numbers here, 1 and 3. When we add these together, we get:\\n\\n1 (the first number) + 3 (the second number) = 4\\n\\nSo, the answer to \"What is the sum of 1 and 3?\" is 4.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import GPT4All\n",
    "from langchain_core.callbacks import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate,ChatPromptTemplate,SystemMessagePromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = GPT4All(model=\"../GPT4ALL-Models/mistral-7b-openorca.gguf2.Q4_0.gguf\",callbacks=callbacks, verbose=True)\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt = prompt,\n",
    "    output_key ='answer'\n",
    ")\n",
    "result = chain({\n",
    "    \"question\":\"Could you please tell me What is the answer of 1 + 3\", \n",
    "})\n",
    "print(\"\\n >>>>>>>>>>>>>>>>>>>>>>>> Result: \\n\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
